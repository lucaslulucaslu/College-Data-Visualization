{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b328604-e10f-4c1c-9171-c315c014ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from LLM_get_folder import get_local_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72238d6d-77a9-4f2b-a8a7-cc938a46b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_url_database(urls=[]):\n",
    "    connection = pymysql.connect(\n",
    "        db=os.environ[\"db_name\"],\n",
    "        user=os.environ[\"db_user\"],\n",
    "        passwd=os.environ[\"db_pass\"],\n",
    "        host=os.environ[\"db_host\"],\n",
    "        port=3306,\n",
    "        cursorclass=pymysql.cursors.DictCursor,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    insert_count = 0\n",
    "    for url in urls:\n",
    "        query = \"SELECT ID FROM fp_chatGPT.news_urls WHERE url LIKE %s\"\n",
    "        rows_count = cursor.execute(query, url)\n",
    "        if rows_count == 0:\n",
    "            query = \"\"\"INSERT INTO `fp_chatGPT`.`news_urls` (`url`) VALUES (%s)\"\"\"\n",
    "            cursor.execute(query, url)\n",
    "            insert_count = insert_count + 1\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    return insert_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bfbdd2-f894-473d-af84-53aca0597a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_posts():\n",
    "    connection = pymysql.connect(\n",
    "        db=os.environ[\"db_name\"],\n",
    "        user=os.environ[\"db_user\"],\n",
    "        passwd=os.environ[\"db_pass\"],\n",
    "        host=os.environ[\"db_host\"],\n",
    "        port=3306,\n",
    "        cursorclass=pymysql.cursors.DictCursor,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    query=\"SELECT COUNT(ID) as number_left FROM fp_chatGPT.news_urls WHERE post IS NULL\"\n",
    "    cursor.execute(query)\n",
    "    row=cursor.fetchone()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    return row['number_left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53524f6-0d16-46a7-8dd6-934016afd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"\n",
    "\n",
    "\n",
    "def Northeastern():\n",
    "    categories = [\n",
    "        \"https://news.northeastern.edu/category/university-news/\",\n",
    "        \"https://news.northeastern.edu/category/world-news/\",\n",
    "        \"https://news.northeastern.edu/category/science-technology/\",\n",
    "        \"https://news.northeastern.edu/category/arts-entertainment/\",\n",
    "        \"https://news.northeastern.edu/category/business/\",\n",
    "        \"https://news.northeastern.edu/category/health/\",\n",
    "        \"https://news.northeastern.edu/category/law/\",\n",
    "        \"https://news.northeastern.edu/category/lifestyle/\",\n",
    "        \"https://news.northeastern.edu/category/society-culture/\",\n",
    "        \"https://news.northeastern.edu/category/sports/\",\n",
    "    ]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\".results-list .results-list__grid .post-link > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Princeton():\n",
    "    categories = [\"https://www.princeton.edu/news\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.news-run.item > div > div > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def MIT():\n",
    "    categories = [\n",
    "        \"https://news.mit.edu/school/engineering\",\n",
    "        \"https://news.mit.edu/school/humanities-arts-and-social-sciences\",\n",
    "        \"https://news.mit.edu/school/architecture-and-planning\",\n",
    "        \"https://news.mit.edu/school/management\",\n",
    "        \"https://news.mit.edu/school/science\",\n",
    "        \"https://news.mit.edu/school/mit-schwarzman-college-computing\",\n",
    "    ]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"a.term-page--news-article--item--title--link\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Harvard():\n",
    "    categories = [\"https://news.harvard.edu/gazette/section/news_plus/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h2.hentry__title.wp-block-post-title > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Standford_med():\n",
    "    categories = [\"https://med.stanford.edu/news.html\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.col-xs-9 > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Yale():\n",
    "    categories = [\"https://news.yale.edu/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.view > div.cards > article.card__item > a.card__body\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Chicago():\n",
    "    categories = [\"https://news.uchicago.edu/lateststories\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"a.c-list__item-titlelink\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def JHU():\n",
    "    categories = [\"https://hub.jhu.edu/university-news/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\n",
    "            \"div.count-10 > div.force > div.text > div.accent-element > h5 > a\"\n",
    "        )\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UPenn():\n",
    "    categories = [\"https://penntoday.upenn.edu/news/archives\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"a.search-tease__link\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Caltech():\n",
    "    categories = [\"https://www.caltech.edu/about/news\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\n",
    "            \".block-NewsArticleListBlock .news-article-list .article-teaser__image-container a\"\n",
    "        )\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Duke():\n",
    "    categories = [\"https://today.duke.edu/trending\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.text-container > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Northwestern():\n",
    "    categories = [\"https://news.northwestern.edu/stories/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h3 > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Dartmouth():\n",
    "    categories = [\"https://home.dartmouth.edu/news?content-types=article#articles-list\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\".views-row article a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Brown():\n",
    "    categories = [\"https://www.brown.edu/news/all\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\".news-items .news .component_body h3 a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Rice():\n",
    "    categories = [\"https://news.rice.edu/news-releases\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"a.article__news-list-summary-label\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def WUSTL():\n",
    "    categories = [\"https://source.wustl.edu/channels/newsroom/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"a.link-animate\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Cornell():\n",
    "    categories = [\"https://news.cornell.edu/archive\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h2 > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def NotreDame():\n",
    "    categories = [\"https://news.nd.edu/latest-news/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"a.card-link\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UCB():\n",
    "    categories = [\"https://news.berkeley.edu/all-news/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"article.preview--post .preview__body h2 a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UCLA():\n",
    "    categories = [\"https://newsroom.ucla.edu/stories\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h3.article-list-title > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def CMU():\n",
    "    categories = [\"https://www.cmu.edu/news/stories\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"a.link-box\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Emory():\n",
    "    categories = [\"https://news.emory.edu/tags/topic/news_releases/index.html\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"li.tag-list-item > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Georgetown():\n",
    "    categories = [\"https://www.georgetown.edu/news/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\".c--articles h3 a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UMich():\n",
    "    categories = [\"https://news.umich.edu/all-stories/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"li.clearfix > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def USC():\n",
    "    categories = [\"https://today.usc.edu/tag/usc-news/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"article h3>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Virginia():\n",
    "    categories = [\"https://news.virginia.edu/content/all-news\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.uva-today-news-item-title > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UFL():\n",
    "    categories = [\n",
    "        \"https://news.ufl.edu/science/\",\n",
    "        \"https://news.ufl.edu/life/\",\n",
    "        \"https://news.ufl.edu/health/\",\n",
    "        \"https://news.ufl.edu/campus/\",\n",
    "    ]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\".article-listing .listing-card .h5 a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def WFU():\n",
    "    categories = [\"https://news.wfu.edu/archive/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h3 > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Tufts():\n",
    "    categories = [\n",
    "        \"https://now.tufts.edu/activism-social-justice\",\n",
    "        \"https://now.tufts.edu/arts-humanities\",\n",
    "        \"https://now.tufts.edu/business-economics\",\n",
    "        \"https://now.tufts.edu/climate-sustainability\",\n",
    "        \"https://now.tufts.edu/food-nutrition\",\n",
    "        \"https://now.tufts.edu/global-affairs\",\n",
    "        \"https://now.tufts.edu/health\",\n",
    "        \"https://now.tufts.edu/animal-health-medicine\",\n",
    "        \"https://now.tufts.edu/points-view\",\n",
    "        \"https://now.tufts.edu/politics-voting\",\n",
    "        \"https://now.tufts.edu/science-technology\",\n",
    "    ]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h3.teaser__title > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UCSB():\n",
    "    categories = [\"https://news.ucsb.edu/all-news\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"span.field-content > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UCI():\n",
    "    categories = [\n",
    "        \"https://news.uci.edu/category/art-and-humanities/\",\n",
    "        \"https://news.uci.edu/category/athletics/\",\n",
    "        \"https://news.uci.edu/category/campus-life/\",\n",
    "        \"https://news.uci.edu/category/health/\",\n",
    "        \"https://news.uci.edu/category/science-and-tech/\",\n",
    "        \"https://news.uci.edu/category/society-and-community/\",\n",
    "    ]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h3.posts-grid-wide__post-title > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UCSD():\n",
    "    categories = [\"https://today.ucsd.edu/archives\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h3.h4 > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def BostonCollege():\n",
    "    categories = [\"https://www.bc.edu/content/bc-web/sites/bc-news.html\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\".bc-news-article-list-2022 .article-block>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def BostonCollege():\n",
    "    categories = [\"https://www.bc.edu/content/bc-web/sites/bc-news.html\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\".bc-news-article-list-2022 .article-block>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Rochester():\n",
    "    categories = [\n",
    "        \"https://www.rochester.edu/newscenter/category/news/sci-tech/\",\n",
    "        \"https://www.rochester.edu/newscenter/category/news/society-culture/\",\n",
    "        \"https://www.rochester.edu/newscenter/category/news/university-news/\",\n",
    "    ]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\".loop-item__title>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UCDavis():\n",
    "    categories = [\"https://www.ucdavis.edu/news/latest\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h3.vm-teaser__title > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UTexas():\n",
    "    categories = [\"https://news.utexas.edu/archive/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h2.ut-h3 > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def WISC():\n",
    "    categories = [\"https://news.wisc.edu/latest-news/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h2.headline > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def BostonU():\n",
    "    categories = [\"https://www.bu.edu/today/browse/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"a.wp-prepress-component-story-card-link\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UIUC():\n",
    "    categories = [\"https://news.illinois.edu/view/6367?ACTION=POST_LIST\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h3.blog-post-title > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def WilliamMary():\n",
    "    categories = [\"https://news.wm.edu/latest-news/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"#main article h2 a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Brandeis():\n",
    "    categories = [\"https://www.brandeis.edu/stories/2024/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\".wrap.wrap--xslim .feed .h6 a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Case():\n",
    "    categories = [\n",
    "        \"https://thedaily.case.edu/category/campus-community/\",\n",
    "        \"https://thedaily.case.edu/category/university-news/\",\n",
    "        \"https://thedaily.case.edu/category/research/\",\n",
    "        \"https://thedaily.case.edu/category/people/\",\n",
    "    ]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h2.title > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Rutgers():\n",
    "    categories = [\"https://www.rutgers.edu/news/all\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"article.cc--news-card  h3 > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Washington():\n",
    "    categories = [\"https://www.washington.edu/news/the-latest-news-from-the-uw/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h2 > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Purdue():\n",
    "    categories = [\"https://www.purdue.edu/newsroom/archive.html\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.col-sm-8 > div > ul > li > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UMaryland():\n",
    "    categories = [\"https://today.umd.edu/articles-list\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h2 > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def TAMU():\n",
    "    categories = [\"https://today.tamu.edu/archives/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"span.headline-group__head > span > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def VT():\n",
    "    categories = [\"https://news.vt.edu/articles.html\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"ul.vt-list-items li .vt-list-item-row .vt-list-title a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def Lehigh():\n",
    "    categories = [\"https://www2.lehigh.edu/news/all-stories\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.col-md-8 > a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def OhioState():\n",
    "    categories = [\"https://news.osu.edu/?h=1&t=News,Research%20News\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"h3.pp_bigheadlines_heading>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UofGeorgia():\n",
    "    categories = [\"https://news.uga.edu/2024/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\".entry-header h2.entry-title>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UofConnecticut():\n",
    "    categories = [\"https://today.uconn.edu/archives/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"ul.archive-list li div.uct-small-story-row-headline>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def MichiganState():\n",
    "    categories = [\"https://msutoday.msu.edu/view-all\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.filtered-content div.row div.col-md-4 a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def PennState():\n",
    "    categories = [\"https://www.psu.edu/news/latest-news/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"article a#latest-news\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def NorthCarolinaState():\n",
    "    categories = [\"https://news.ncsu.edu/2024/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.index-listing>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def SyracuseU():\n",
    "    categories = [\"https://news.syr.edu/sections/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.article-list div.post h2.article-title a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def UofMiami():\n",
    "    categories = [\"https://news.miami.edu/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.news_container h2.stagsansmedium>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def TempleU():\n",
    "    categories = [\"https://news.temple.edu/news/archives\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"div.view-content div.views-row div.views-field-title a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def ClemsonU():\n",
    "    categories = [\"https://news.clemson.edu/topics/all-stories/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"#main article>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "\n",
    "def FordhamU():\n",
    "    categories = [\"https://now.fordham.edu/2024/\"]\n",
    "    n = 0\n",
    "    for category in categories:\n",
    "        response = requests.get(category, headers={\"User-agent\": user_agent})\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = soup.select(\"article .content h2.post-title>a\")\n",
    "        links_array = [urljoin(category, link.get(\"href\")) for link in links]\n",
    "        n = n + insert_url_database(links_array)\n",
    "        time.sleep(3)\n",
    "    return n\n",
    "\n",
    "def XXX():\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54fcaeb-09fe-45f0-bd23-ea2333c5dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colleges = [\n",
    "    Lehigh,\n",
    "    VT,\n",
    "    TAMU,\n",
    "    UMaryland,\n",
    "    Purdue,\n",
    "    Washington,\n",
    "    Rutgers,\n",
    "    Case,\n",
    "    Brandeis,\n",
    "    WilliamMary,\n",
    "    UIUC,\n",
    "    BostonU,\n",
    "    WISC,\n",
    "    UTexas,\n",
    "    UCDavis,\n",
    "    Rochester,\n",
    "    BostonCollege,\n",
    "    UCSD,\n",
    "    UCI,\n",
    "    UCSB,\n",
    "    Tufts,\n",
    "    UFL,\n",
    "    WFU,\n",
    "    Virginia,\n",
    "    USC,\n",
    "    UMich,\n",
    "    Georgetown,\n",
    "    Emory,\n",
    "    Northeastern,\n",
    "    Princeton,\n",
    "    MIT,\n",
    "    Harvard,\n",
    "    Standford_med,\n",
    "    Yale,\n",
    "    Chicago,\n",
    "    JHU,\n",
    "    UPenn,\n",
    "    Caltech,\n",
    "    Duke,\n",
    "    Northwestern,\n",
    "    Dartmouth,\n",
    "    Brown,\n",
    "    Rice,\n",
    "    WUSTL,\n",
    "    Cornell,\n",
    "    NotreDame,\n",
    "    UCB,\n",
    "    UCLA,\n",
    "    CMU,\n",
    "    OhioState,\n",
    "    UofGeorgia,\n",
    "    UofConnecticut,\n",
    "    MichiganState,\n",
    "    PennState,\n",
    "    NorthCarolinaState,\n",
    "    SyracuseU,\n",
    "    UofMiami,\n",
    "    TempleU,\n",
    "    ClemsonU,\n",
    "    FordhamU\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dec335-62c2-49d6-9c1e-b02ea63a8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=get_local_folder()\n",
    "filepath=os.path.join(folder,\"urls.csv\")\n",
    "df=pd.read_csv(filepath)\n",
    "temp={}\n",
    "for college in colleges:\n",
    "    n=college()\n",
    "    temp[college.__name__]=n\n",
    "    print(college.__name__,n)\n",
    "temp['number_left']=get_left_posts()\n",
    "df2=pd.DataFrame([temp])\n",
    "df=pd.concat([df,df2]).reset_index(drop=True)\n",
    "df.to_csv(filepath,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff05c4-0463-4579-9511-84901f92cfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
